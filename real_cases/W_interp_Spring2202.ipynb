{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17803d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os as os\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob\n",
    "import netCDF4 as ncdf\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Local import \n",
    "\n",
    "from spectra.py_spectra import *\n",
    "\n",
    "# Two levels that are used to create mean analysis\n",
    "\n",
    "analysis_levels  = [6,17,28]\n",
    "analysis_levels  = [10,25]\n",
    "\n",
    "# These are 45 vertical levels that the FV3 puts out - use them here to map ARW to that grid for comparison\n",
    "\n",
    "plevels = np.asarray([100000.,  97500.,  95000.,  92500.,  90000.,  87500.,  85000.,  82500.,\n",
    "                       80000.,  77500.,  75000.,  72500.,  70000.,  67500.,  65000.,  62500.,\n",
    "                       60000.,  57500.,  55000.,  52500.,  50000.,  47500.,  45000.,  42500.,\n",
    "                       40000.,  37500.,  35000.,  32500.,  30000.,  27500.,  25000.,  22500.,\n",
    "                       20000.,  17500.,  15000.,  12500.,  10000.,   7000.,   5000.,   3000.,\n",
    "                        2000.,   1000.,    700.,    500.,    200.])\n",
    "\n",
    "plevels = np.asarray([75000.,  72500.,  70000.,  67500.,  65000.,\n",
    "                      37500.,  35000.,  32500.,  30000.,  27500.])\n",
    "\n",
    "zlevels = (1500., 1750., 2000., 2250., 2500., 8500., 8750., 9000., 9250., 9500.)\n",
    "zlevels = 1000. + 250.*np.arange(45)\n",
    "\n",
    "# Helper functions......\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# Interp from 3D pressure to 1D pressure (convert from hybrid to constant p-levels)\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def interp3d_np(data, p3d, p1d, debug=True):\n",
    "    \n",
    "    dinterp = np.zeros((len(p1d),data.shape[1],data.shape[2]),dtype=np.float32)\n",
    "    \n",
    "    # if debug:\n",
    "    #     print(\"Input  data at %d, Max/Min:  (%10.4g, %10.4g)\" % (n,data.max(), data.min()))\n",
    "\n",
    "    for i in np.arange(data.shape[2]):\n",
    "        for j in np.arange(data.shape[1]):\n",
    "            dinterp[:,j,i] = np.interp(p1d, p3d[:,j,i], data[:,j,i])\n",
    "            \n",
    "    # if debug:\n",
    "    #     print(\"Output data at %d, Max/Min:  (%10.4g, %10.4g)\\n\" % (n,dinterp[n].max(), dinterp[n].min()))\n",
    " \n",
    "    return dinterp\n",
    "\n",
    "@jit(nopython=True)\n",
    "def interp4d_np(data, p3d, p1d, debug=False):\n",
    "        \n",
    "    dinterp = np.zeros((data.shape[0],len(p1d),data.shape[2],data.shape[3]),dtype=np.float32)\n",
    "    \n",
    "    for n in np.arange(data.shape[0]):\n",
    "        \n",
    "        # if debug:\n",
    "        #     print(\"Input  data at %d, Max/Min:  (%10.4g, %10.4g)\" % (n,data[n].max(), data[n].min()))\n",
    "        for i in np.arange(data.shape[3]):\n",
    "            for j in np.arange(data.shape[2]):\n",
    "                dinterp[n,:,j,i] = np.interp(p1d, p3d[n,:,j,i], data[n,:,j,i])\n",
    "        # if debug:\n",
    "        #     print(\"Output data at %d, Max/Min:  (%10.4g, %10.4g)\\n\" % (n,dinterp[n].max(), dinterp[n].min()))\n",
    "    \n",
    "    return dinterp\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#   \n",
    "def add_fhour(ds, debug=False):\n",
    "        \n",
    "    DateAndTime = os.path.split(ds.encoding[\"source\"])[1]  # this gets the filename from the directory\n",
    "    \n",
    "    if debug == True:\n",
    "            print(\"Filename to be parsed: \", DateAndTime)\n",
    "    \n",
    "    DT_obj = datetime.strptime(DateAndTime.split(\"_\")[0], \"%Y%m%d%H%M\") # this converts the leading YYYYMMDDHHMM\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"Date Time Object from filename: \", DT_obj)\n",
    "    \n",
    "    init_obj = datetime.strptime(ds.date, \"%Y%m%d%H\")   # this gets the initialization date & time attribute from the file \n",
    "\n",
    "    if debug == True:\n",
    "        print(\"Date Time Object from initialization: \", init_obj)\n",
    "\n",
    "    fhour    = int((DT_obj - init_obj).seconds / 3600.0)  # this does a time delta and divides into hours\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"Time in hours of forecast: \", init_obj)\n",
    "\n",
    "    ds.coords['fhour']     = fhour              # attach this attribute to the dataset\n",
    "    ds.coords['init_time'] = init_obj           # attach this attribute to the dataset\n",
    "    \n",
    "    return ds\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#   \n",
    "def open_mfdataset_list(data_dir, pattern, debug=False):\n",
    "    \"\"\"\n",
    "    Use xarray.open_mfdataset to read multiple netcdf files from a list.\n",
    "    \"\"\"\n",
    "    filelist = sorted(glob.glob(os.path.join(data_dir,pattern)))\n",
    "    \n",
    "    if debug == True:\n",
    "        print(filelist)\n",
    "    \n",
    "    return xr.open_mfdataset(filelist, preprocess=add_fhour, combine='nested', concat_dim=['fhour'],parallel=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c66127-e753-4680-a662-b4382d571edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate u, v, w\n",
    "\n",
    "def interp_fields(in_dir, day, out_dir):\n",
    "    \n",
    "    hrrr_dir  = str(os.path.join(in_dir, day, \"hrrr\"))\n",
    "    rrfs_dir = str(os.path.join(in_dir, day, \"rrfs_b\"))\n",
    "\n",
    "    hrrr = open_mfdataset_list(hrrr_dir , \"*HRRR_ECONUS.nc\")\n",
    "    rrfs = open_mfdataset_list(rrfs_dir, \"*RRFSB_ECONUS.nc\")\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    u_hrrr = interp4d_np(np.nan_to_num(hrrr.u.values).astype('float32'), \n",
    "                         np.nan_to_num(hrrr.gh.values).astype('float32'), zlevels)\n",
    "    v_hrrr = interp4d_np(np.nan_to_num(hrrr.v.values).astype('float32'), \n",
    "                         np.nan_to_num(hrrr.gh.values).astype('float32'), zlevels)\n",
    "    w_hrrr = interp4d_np(np.nan_to_num(hrrr.wz.values).astype('float32'), \n",
    "                         np.nan_to_num(hrrr.gh.values).astype('float32'), zlevels)\n",
    "    d_hrrr = interp4d_np(np.nan_to_num(hrrr.refl10cm.values).astype('float32'), \n",
    "                         np.nan_to_num(hrrr.gh.values).astype('float32'), zlevels)\n",
    "    \n",
    "    print(\"HRRR file interpolated\")\n",
    "    \n",
    "    toc = time.perf_counter()            \n",
    "\n",
    "    print(f\"4D HRRR interp took {toc - tic:0.4f} seconds\\n\")\n",
    "\n",
    "    \n",
    "    ds = xr.Dataset( data_vars=dict(u_interp=(['fhour',\"nz\",\"ny\",\"nx\"], u_hrrr),\n",
    "                                    v_interp=(['fhour',\"nz\",\"ny\",\"nx\"], v_hrrr),\n",
    "                                    w_interp=(['fhour',\"nz\",\"ny\",\"nx\"], w_hrrr),\n",
    "                                  dbz_interp=(['fhour',\"nz\",\"ny\",\"nx\"], d_hrrr)),\n",
    "                     coords={'fhour': ([\"fhour\"],   hrrr.fhour.values),\n",
    "                                 'z': ([\"nz\"],      zlevels),\n",
    "                              \"lons\": ([\"ny\",\"nx\"], hrrr.longitude.values),\n",
    "                              \"lats\": ([\"ny\",\"nx\"], hrrr.latitude.values)},\n",
    "                     attrs=dict(description=\"Interpolated HRRR output to constant heights\",\n",
    "                            date=day))\n",
    "    \n",
    "    outfilename = os.path.join(out_dir, \"%s_HRRR_ECONUS.nc\" % day)\n",
    "    ds.to_netcdf(outfilename, mode='w')\n",
    "    del(ds)\n",
    "\n",
    "    print(\"HRRR file written\")\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    u_rrfs = interp4d_np(np.nan_to_num(rrfs.u.values).astype('float32'), \n",
    "                         np.nan_to_num(rrfs.gh.values).astype('float32'), zlevels)\n",
    "    v_rrfs = interp4d_np(np.nan_to_num(rrfs.v.values).astype('float32'), \n",
    "                         np.nan_to_num(rrfs.gh.values).astype('float32'), zlevels)\n",
    "    w_rrfs = interp4d_np(np.nan_to_num(rrfs.wz.values).astype('float32'), \n",
    "                         np.nan_to_num(rrfs.gh.values).astype('float32'), zlevels)\n",
    "    d_rrfs = interp4d_np(np.nan_to_num(rrfs.refl10cm.values), \n",
    "                         np.nan_to_num(rrfs.gh.values).astype('float32'), zlevels)\n",
    "    \n",
    "    print(\"RRFS file interpolated\")\n",
    "    \n",
    "    toc = time.perf_counter()            \n",
    "\n",
    "    print(f\"4D RRFS interp took {toc - tic:0.4f} seconds\\n\")\n",
    "\n",
    "    ds = xr.Dataset( data_vars=dict(u_interp=(['fhour',\"nz\",\"ny\",\"nx\"], u_rrfs),\n",
    "                                    v_interp=(['fhour',\"nz\",\"ny\",\"nx\"], v_rrfs),\n",
    "                                    w_interp=(['fhour',\"nz\",\"ny\",\"nx\"], w_rrfs),\n",
    "                                  dbz_interp=(['fhour',\"nz\",\"ny\",\"nx\"], d_rrfs)),\n",
    "                 coords={'fhour': ([\"fhour\"],   hrrr.fhour.values),\n",
    "                             'z': ([\"nz\"],      zlevels),\n",
    "                          \"lons\": ([\"ny\",\"nx\"], hrrr.longitude.values),\n",
    "                          \"lats\": ([\"ny\",\"nx\"], hrrr.latitude.values)},\n",
    "                 attrs=dict(description=\"Interpolated HRRR output to constant heights\",\n",
    "                            date=day))\n",
    "    \n",
    "    \n",
    "    outfilename = os.path.join(out_dir, \"%s_RRFS_ECONUS.nc\" % day)\n",
    "    ds.to_netcdf(outfilename, mode='w')\n",
    "    del(ds)\n",
    "    \n",
    "    print(\"RRFS file written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16bfb6-bf64-4085-aa8a-dd34e9575a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing day:  2022050400\n",
      "HRRR file interpolated\n",
      "4D HRRR interp took 46.5567 seconds\n",
      "\n",
      "HRRR file written\n",
      "RRFS file interpolated\n",
      "4D RRFS interp took 93.8624 seconds\n",
      "\n",
      "RRFS file written\n",
      "\n",
      "Processing day:  2022051200\n"
     ]
    }
   ],
   "source": [
    "in_dir  = \"/work/larissa.reames\"\n",
    "out_dir = \"/work/wicker/CAM_analysis_tools\"\n",
    "case_days = [\"2022050400\",\n",
    "            \"2022051200\",\n",
    "            \"2022051400\",\n",
    "            \"2022051500\",\n",
    "            \"2022051900\",\n",
    "            \"2022052300\",\n",
    "            \"2022052400\",\n",
    "            \"2022052700\",\n",
    "            \"2022053000\",\n",
    "            \"2022060700\"]\n",
    "\n",
    "for day in case_days:\n",
    "    print(\"\\nProcessing day:  %s\" % day)\n",
    "    ret = interp_fields(in_dir, day, out_dir)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6cef7-666c-41e6-bc68-03847a96fa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
